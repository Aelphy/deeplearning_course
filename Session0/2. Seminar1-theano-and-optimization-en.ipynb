{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano, Lasagne\n",
    "and why they matter\n",
    "\n",
    "\n",
    "### got no lasagne?\n",
    "Install the __bleeding edge__ version from here: http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares(N):\n",
    "    return <student.Implement_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I gonna be function parameter\n",
    "N = T.scalar('a dimension', dtype='int32')\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#Compiling the recipe of computing \"result\" given N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.scalar()\n",
    "\n",
    "f = theano.function([x], x / x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "__if you're currently in classroom, chances are i am explaining this text wall right now__\n",
    "* 1 You define inputs f your future function;\n",
    "* 2 You write a recipe for some transformation of inputs;\n",
    "* 3 You compile it;\n",
    "* You have just got a function!\n",
    "* The gobbledegooky version: you define a function as symbolic computation graph.\n",
    "\n",
    "\n",
    "* There are two main kinds of entities: \"Inputs\" and \"Transformations\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be integers, floats of booleans (uint8) of various size.\n",
    "\n",
    "\n",
    "* An input is a placeholder for function parameters.\n",
    " * N from example above\n",
    "\n",
    "\n",
    "* Transformations are the recipes for computing something given inputs and transformation\n",
    " * (T.arange(N)^2).sum() are 3 sequential transformations of N\n",
    " * Doubles all functions of numpy vector syntax\n",
    " * You can almost always go with replacing \"np.function\" with \"T.function\" aka \"theano.tensor.function\"\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * and so on.\n",
    "   * builtin operations also work that way\n",
    "   * np.arange(10).mean() -> T.arange(10).mean()\n",
    "   * Once upon a blue moon the functions have different names or locations (e.g. T.extra_ops)\n",
    "     * Ask us or google it\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "example_input_integer = T.scalar('scalar input', dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4('four dimensional tensor input') #dtype = theano.config.floatX by default\n",
    "#do not warry, we won't need tensor\n",
    "#yet\n",
    "\n",
    "input_vector = T.vector('', dtype='int32') # vector of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "#transofrmation: elementwise multiplication\n",
    "double_the_vector = input_vector * 2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector**2 - input_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Practice time:\n",
    "#create two vectors of size float32\n",
    "my_vector = student.init_float32_vector()\n",
    "my_vector2 = student.init_one_more_such_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = student.implementwhatwaswrittenabove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(my_transformation)\n",
    "#it's okay it aint a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling\n",
    "* So far we were using \"symbolic\" variables and transformations\n",
    " * Defining the recipe for computation, but not computing anything\n",
    "* To use the recipe, one should compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [<two vectors that my_transformation depends on>]\n",
    "outputs = [<What do we compute (can be a list of several transformation)>]\n",
    "\n",
    "# The next lines compile a function that takes two vectors and computes your transformation\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using function with, lists:\n",
    "print('using python lists:')\n",
    "print(my_function([1, 2, 3], [4, 5, 6]))\n",
    "print()\n",
    "\n",
    "#Or using numpy arrays:\n",
    "#btw, that 'float' dtype is casted to secong parameter dtype which is float32\n",
    "print('using numpy arrays:')\n",
    "print(my_function(np.arange(10),\n",
    "                  np.linspace(5, 6, 10, dtype='float')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "* Compilation can take a while for big functions\n",
    "* To avoid waiting, one can evaluate transformations without compiling\n",
    "* Without compilation, the code runs slower, so consider reducing input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a dictionary of inputs\n",
    "my_function_inputs = {\n",
    "    my_vector:[1, 2, 3],\n",
    "    my_vector2:[4, 5, 6]\n",
    "}\n",
    "\n",
    "# evaluate my_transformation\n",
    "# has to match with compiled function output\n",
    "print(my_transformation.eval(my_function_inputs))\n",
    "\n",
    "\n",
    "# can compute transformations on the fly\n",
    "print('add 2 vectors', (my_vector + my_vector2).eval(my_function_inputs))\n",
    "\n",
    "\n",
    "#!WARNING! if your transformation only depends on some inputs,\n",
    "#do not provide the rest of them\n",
    "print('vector\\'s shape:', my_vector.shape.eval({\n",
    "                            my_vector: [1, 2, 3]\n",
    "                          })\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When debugging, one would generally want to reduce the computation complexity. For example, if you are about to feed neural network with 1000 samples batch, consider taking first 2.\n",
    "* If you really want to debug graph of high computation complexity, you could just as well compile it (e.g. with optimizer='fast_compile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do It Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "<student.define_inputs_and_transformations()>\n",
    "\n",
    "compute_mse =<student.compile_function()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1, 5, 10, 10**3]:\n",
    "    elems = [np.arange(n), np.arange(n, 0, -1), np.zeros(n),\n",
    "             np.ones(n), np.random.random(n), np.random.randint(100, size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el, el_2))\n",
    "            my_mse = compute_mse(el, el_2)\n",
    "            \n",
    "            if not np.allclose(true_mse, my_mse):\n",
    "                print('Wrong result:')\n",
    "                print('mse({},{})'.format(el, el_2))\n",
    "                print('should be: {}, but your function returned {}'.format(true_mse, my_mse))\n",
    "                raise ValueError, 'Smth went wrong'\n",
    "\n",
    "print('All tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared variables\n",
    "\n",
    "* The inputs and transformations only exist when function is called\n",
    "\n",
    "* Shared variables always stay in memory like global variables\n",
    " * Shared variables can be included into a symbolic graph\n",
    " * They can be set and evaluated using special methods\n",
    "   * but they can't change value arbitrarily during symbolic graph computation\n",
    "   * we'll cover that later;\n",
    " \n",
    " \n",
    "* Hint: such variables are a perfect place to store network parameters\n",
    " * e.g. weights or some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = theano.shared(np.ones(10, dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print('initial value', shared_vector_1.get_value())\n",
    "\n",
    "# within symbolic graph you use them just as any other input or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting new value\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#getting that new value\n",
    "print('new value', shared_vector_1.get_value())\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a recipe (transformation) that computes an elementwise transformation of shared_vector and input_scalar\n",
    "#Compile as a function of input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = <student.write_recipe()>\n",
    "\n",
    "shared_times_n = <student.compile_function()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('shared:', shared_vector_1.get_value())\n",
    "\n",
    "print('shared_times_n(5)', shared_times_n(5))\n",
    "\n",
    "print('shared_times_n(-0.5)', shared_times_n(-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Changing value of vector 1 (output should change)\n",
    "shared_vector_1.set_value([-1, 0, 1])\n",
    "\n",
    "print('shared:', shared_vector_1.get_value())\n",
    "\n",
    "print('shared_times_n(5)', shared_times_n(5))\n",
    "\n",
    "print('shared_times_n(-0.5)', shared_times_n(-0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T.grad - why theano matters\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ transformation over one or several scalar or vector (or tensor) transformations or inputs.\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar], scalar_squared)\n",
    "grad = theano.function([my_scalar], derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-3, 3)\n",
    "x_squared = list(map(fun, x))\n",
    "x_squared_der = list(map(grad, x))\n",
    "\n",
    "plt.plot(x, x_squared, label='x^2')\n",
    "plt.plot(x, x_squared_der, label='derivative')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) + 1. / T.arcsinh(my_scalar)).mean() / (my_scalar**2 + 1) + 0.01 * T.sin(2 * my_scalar**1.5) * (T.sum(my_vector) * my_scalar**2) * T.exp((my_scalar - 4)**2) / (1 + T.exp((my_scalar - 4)**2)) * (1. - (T.exp( - (my_scalar - 4)**2)) / (1 + T.exp( - (my_scalar - 4)**2)))**2\n",
    "\n",
    "der_by_scalar, der_by_vector = <student.compute_grad_over_scalar_and_vector()>\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar, my_vector], weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar, my_vector], der_by_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1, 2, 3]\n",
    "\n",
    "scalar_space = np.linspace(0, 7)\n",
    "\n",
    "y = [compute_weird_function(x, vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space, y, label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x, vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space, y_der_by_scalar, label='derivative')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - Updates\n",
    "\n",
    "* updates are a way of changing shared variables at after function call.\n",
    "\n",
    "* technically it's a dictionary {shared_variable : a recipe for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiply shared vector by a number and save the product back into shared vector\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #return vector times scalar\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #and write this same result bach into shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#initial shared_vector_1\n",
    "print('initial shared value:', shared_vector_1.get_value())\n",
    "\n",
    "# evaluating the function (shared_vector_1 will be changed)\n",
    "print('compute_and_save(2) returns', compute_and_save(2))\n",
    "\n",
    "#evaluate new shared_vector_1\n",
    "print('new shared value:', shared_vector_1.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X, y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (1797,)]:[0 1 2 3 4 5 6 7 8 9]\n",
      "X [shape - (1797, 64)]:\n",
      "[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.\n",
      "    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.\n",
      "    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.\n",
      "   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.\n",
      "    0.   0.   6.  13.  10.   0.   0.   0.]\n",
      " [  0.   0.   0.  12.  13.   5.   0.   0.   0.   0.   0.  11.  16.   9.\n",
      "    0.   0.   0.   0.   3.  15.  16.   6.   0.   0.   0.   7.  15.  16.\n",
      "   16.   2.   0.   0.   0.   0.   1.  16.  16.   3.   0.   0.   0.   0.\n",
      "    1.  16.  16.   6.   0.   0.   0.   0.   1.  16.  16.   6.   0.   0.\n",
      "    0.   0.   0.  11.  16.  10.   0.   0.]\n",
      " [  0.   0.   0.   4.  15.  12.   0.   0.   0.   0.   3.  16.  15.  14.\n",
      "    0.   0.   0.   0.   8.  13.   8.  16.   0.   0.   0.   0.   1.   6.\n",
      "   15.  11.   0.   0.   0.   1.   8.  13.  15.   1.   0.   0.   0.   9.\n",
      "   16.  16.   5.   0.   0.   0.   0.   3.  13.  16.  16.  11.   5.   0.\n",
      "    0.   0.   0.   3.  11.  16.   9.   0.]]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "print('y [shape - {}]:{}'.format(str(y.shape), y[:10]))\n",
    "print('X [shape - {}]:'.format(str(X.shape)))\n",
    "print(X[:3])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = <student.code_me()>\n",
    "input_X = <student.code_me()>\n",
    "input_y = <student.code_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <predicted probabilities for input_X>\n",
    "loss = <logistic loss (scalar, mean over sample)>\n",
    "\n",
    "grad = <gradient of loss over model weights>\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <new weights after gradient step> #Implement your favorite stochastic optimization algorithm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_function = <compile function that takes X and y, returns log loss and updates weights>\n",
    "predict_function = <compile function that takes X and computes probabilities of y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print('loss at iter {}:{}'.format(i, loss_i)\n",
    "    print('train auc:', roc_auc_score(y_train,predict_function(X_train)))\n",
    "    print('test auc:', roc_auc_score(y_test,predict_function(X_test)))\n",
    "    \n",
    "print('resulting weights:')\n",
    "          \n",
    "plt.imshow(shared_weights.get_value().reshape(8, -1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "I did such and such, that did that cool thing and my awesome logistic regression bloated out that stuff. Finally, i did that thing and felt like Einstein. That cool article and that kind of weed helped me so much (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPOILERS!\n",
    "\n",
    "## Recommended pipeline\n",
    "* Adapt logistic regression to classify some number against others (e.g. zero vs nonzero)\n",
    "    * Generalize it to multiclass logistic regression.\n",
    "* Instead of weight vector you'll have to use matrix (feature_id x class_id)\n",
    "* softmax (exp over sum of exps) can be implemented manually or as T.nnet.softmax (stable)\n",
    "* probably better to use STOCHASTIC gradient descent (minibatch)\n",
    "     in which case sample should probably be shuffled (or use random subsamples on each iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "314px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "371px",
    "left": "0.989583px",
    "right": "20px",
    "top": "106.997px",
    "width": "249px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
