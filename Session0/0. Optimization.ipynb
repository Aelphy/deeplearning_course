{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Learning</center>\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "## <center> Stohastic Optimization </center>\n",
    "### <center> recap </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### <center>Mikhail Usvyatsov</center>\n",
    "\n",
    "\n",
    "##### Thanks to Alexander Panin, Victor Lempitsky, and Dmitry Vetrov for the materials taken from their courses in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "<br><br><br>\n",
    "### Model: \n",
    "$$ X \\rightarrow Wx + b \\rightarrow Y^{pred} $$\n",
    "### Loss function:\n",
    "$$ L = \\sum_{i} \\left( y - y_i \\right)^2 $$\n",
    "### Quiz 1: what is closed form solution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quiz 1: what is closed form solution ?\n",
    "$$ w=(X^TX)^{âˆ’1}X^Ty $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "<br><br><br>\n",
    "### Model: \n",
    "$$ X \\rightarrow Wx + b \\rightarrow Y^{pred} $$\n",
    "### Loss function:\n",
    "$$ L = \\sum_{i} \\left( y - y_i \\right)^2 $$\n",
    "### Quiz 2: What is iterative solution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quiz 2: What is iterative solution ?\n",
    "\n",
    "$$ \\omega_0 = 0 $$\n",
    "$$ \\omega_{i+1} = \\omega_i - \\alpha \\frac{\\partial L}{\\partial W} $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\sum_i -2x(y_i - (wx_i + b))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "<br><br><br>\n",
    "### Model:\n",
    "\n",
    "$X \\rightarrow Wx + b \\rightarrow $ <img src='http://jeffsuderman.com/wp-content/uploads/2016/08/sigmoid.png' width=\"200\"> $\\rightarrow P \\left(y\\right)$\n",
    "\n",
    "$$ P(y)= \\sigma(Wx+b) $$\n",
    "### Quiz 3: What is the loss function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quiz 3: What is the loss function ?\n",
    "\n",
    "$$ L(x) = - \\sum_{i \\in num\\_examples} \\sum_{j \\in num\\_classes} I[y_i = C_j] \\log\\left(P\\left(C_i | x_i\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quiz 4: Guess the iterative solution! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Update:\n",
    "$$ \\omega_{i+1} = \\omega_i - \\alpha \\frac{\\partial L(x)}{\\partial W} $$\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png' width=\"300\">\n",
    "\n",
    "* $\\alpha$ - learning rate, $\\alpha << 1$\n",
    "* L - objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Is Gradient Descent perfect ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton-Raphson\n",
    "\n",
    "### Update:\n",
    "$$ \\omega_{i+1} = \\omega_i - \\alpha H_L^{-1} \\frac{\\partial L(x)}{\\partial W} $$\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Newton_optimization_vs_grad_descent.svg/1200px-Newton_optimization_vs_grad_descent.svg.png' width=\"200\">\n",
    "\n",
    "* $\\alpha$ - learning rate, $\\alpha << 1$\n",
    "* L - objective function\n",
    "* Red - Newton-Raphson\n",
    "* Green - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Newton-Raphson\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "$${\\mathbf  H}={\\begin{bmatrix}{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac  {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic gradient descent (SGD)\n",
    "\n",
    "### Update:\n",
    "$$ \\omega_{i+1} = \\omega_i - \\alpha E \\frac{\\partial L(x)}{\\partial W} $$\n",
    "\n",
    "<img src='../imgs/sgdvsgd.png' width=\"300\">\n",
    "\n",
    "* E - expectation\n",
    "* Loss function is evaluated in several randomly chosen points\n",
    "* $\\alpha$ should decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGD with momentum\n",
    "\n",
    "<br><br><br>\n",
    "### Idea:\n",
    "\n",
    "introduce inertia into the descent\n",
    "\n",
    "$\\omega_0 = 0$, $\\nu_0 = 0$\n",
    "\n",
    "$$\\nu_{i+1} = \\alpha \\frac{\\partial L}{\\partial W} + \\mu \\nu_i$$\n",
    "$$\\omega_{i+1} = \\omega_i + \\nu_{i + 1}$$\n",
    "\n",
    "![](https://1.bp.blogspot.com/-eVq8WSmhxfE/V1K_MNMTjjI/AAAAAAAAFUo/Si6N7fkGErQO3aRitHsY_xTDyABDORU_gCLcB/s1600/momentum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some other heuristic-based algorithms\n",
    "\n",
    "![](http://2.bp.blogspot.com/-q6l20Vs4P_w/VPmIC7sEhnI/AAAAAAAACC4/g3UOUX2r_yA/s1600/s25RsOr%2B-%2BImgur.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thank you. Questions ???\n",
    "\n",
    "![](http://amorebeautifulquestion.com/wp-content/uploads/2012/12/EintsteinQuestionEverything.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
